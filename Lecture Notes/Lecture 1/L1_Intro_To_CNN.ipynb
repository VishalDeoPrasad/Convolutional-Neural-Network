{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q02lGPiXKwgT"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVsL9QWM0Nb7"
      },
      "source": [
        "#### **Agenda**\n",
        "In This Lecture, We Will Cover:\n",
        "- A brief introduction to Computer Vision field.\n",
        "- Why Vanilla NN is **NOT** suited for Image Data? Explained with a real-world example!\n",
        "- Taking motivations for a new architecture from **Human Visual Cortex System**.\n",
        "- Defining the new architecture, aka **CNNs** and its components!\n",
        "\n",
        "Helpful Link: https://poloclub.github.io/cnn-explainer/#article-convolution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDY9n0B253jo"
      },
      "source": [
        "####Q. **What is Computer Vision (CV)?**\n",
        "\n",
        "Simply put, Computer Vision is a field of AI that enables machine to derive **Meaningful Information from Images**, Some Examples:\n",
        "\n",
        "- Assigning classes to an entire image\n",
        "><img src='https://drive.google.com/uc?id=1SzV87D0Gh3fYptX0fI4Z5wcq3urkFEyk' height=250>\n",
        "\n",
        "- Detect all the **Objects** in the Image -\n",
        "><img src='https://drive.google.com/uc?id=1kPFldAGjtfyLm7msBNf9mf5cWsMXLJp5' height=300>\n",
        "\n",
        "\n",
        "- Extract **Text** from the Image -\n",
        "><img src='https://drive.google.com/uc?id=1c9nP2lWR5ig_IMda1vhyIJcmegxANOKp' width=300>\n",
        "\n",
        "\n",
        "- Write one-line describing the Image(known as **Image Captioning**) -\n",
        "><img src='https://drive.google.com/uc?id=1uH7SKPJco1O0tFU7Rs6uZOuJvKVG8EEE' height=300>\n",
        "\n",
        "- Generating images\n",
        "><img src='https://drive.google.com/uc?id=1N6h5b370C_RjkSfc7YWudkOMIFs16c_7' height=300>\n",
        "\n",
        "\n",
        "These problems might be easy for humans as we are trained with many years of image data, but What about Computers?\n",
        "\n",
        "Before even answering this question, let's investigate - <br>\n",
        "###**What a Computer actually sees?**<br>\n",
        "- A **Digital Image** is a matrix of size (H,W,C), comprising of numbers (also known as pixel values) typically ranging from 0-255. Here H,W,C denote Height, Width and No. of Channels in a image.\n",
        "- For a Grayscale image, # channels is 1, while for Colored Image its 3.\n",
        "\n",
        "><img src='https://drive.google.com/uc?id=1v9Q1ivDG7niiOB00qnKDhzvs_iJKHFY5' height=300>\n",
        "\n",
        "><img src='https://drive.google.com/uc?id=1q8RHb2Lt25yvzpojTl4xu8OqroazfxO2' height=300>\n",
        "\n",
        "####Q. **Is CV Easy?**\n",
        "We used Google's Vision API to find out what it detects in the above image.   \n",
        "And the result was ...**...BAD**\n",
        "><img src='https://drive.google.com/uc?id=1Aoc_MLvUNxVGrddvCvKyzeHxfPfGZ6FU' height=450>\n",
        "\n",
        "####Q. **CV is Hard, How?**  \n",
        "- Occlusion\n",
        "><img src='https://drive.google.com/uc?id=1BEPnFwfEHbyS9B0mGuwH3PrXoSD3hD3p' height=350>\n",
        "- Illumination variability\n",
        "><img src='https://drive.google.com/uc?id=1lCwC1MQ3J8oLEI8wVUUASvEDezyvzxny' height=350>\n",
        "- Pose Variability\n",
        "><img src='https://drive.google.com/uc?id=1EomrGXii9kUWuFBAdvj97VnGY5Gs57JZ' height=350>\n",
        "\n",
        "\n",
        "**Scope of CV is Enormous!**\n",
        "- Computer vision is used in industry leading products like self-driving cars, automated robots, drones, medical image analysis, etc.\n",
        "- Apart from this, CV is also used in conjunction with other fields of AI like Speech and NLP.\n",
        "- For example, in the case of Speech Recognition, we first convert audio chunks to 2D matrices (called mel spectrogram) and treat it like images. In NLP, we use CV techniques for Image Captioning and very recently Image Synthesis from prompts.   \n",
        "\n",
        ">*Note that the above image of dog is artificially generated using [Stable Diffusion](https://huggingface.co/spaces/stabilityai/stable-diffusion) model, with the following prompt: `A dog made of origami. Highly detailed. Photograph. 4k. Colorful`*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvIudBOwxv0U"
      },
      "source": [
        "####Q. **Why Deep Neural Network for Images?**\n",
        "\n",
        "In order to understand why DNN works, we need to account for how visual cortex functions!\n",
        "\n",
        "Our Visual Cortex system is arranged in layers and as information passes from the eyes to deeper parts of the brain, higher and higher order representations are formed. Since a DNN also works in a similar way, using it for analyzing Images makes perfect sense.\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1IkIqSSDHlSLeVDHdu1n-ZsHbiqMs5nAe' height=550>\n",
        "\n",
        "####**But what about the architecture of DNN, will Vanilla NN or MLP be a suitable fit?**\n",
        "\n",
        "### We will answer this question in Part 1 of the module!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45RZ3hmDpL2e"
      },
      "source": [
        "# PART 1: Hammer and Screw\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1O94kFYu6pvDMeNuWKQ7HKgwNlFgllNAV' height=280 width=400>\n",
        "\n",
        "\n",
        "In the Previous lectures we learnt about ANNs and where to use them.   \n",
        "\n",
        "Now let's experiment what happens if we try to fit a Vanilla Neural Network (Hammer) to a very different kind of data, aka Images (Screw) and why this hammering of screw doesn't make sense!\n",
        "\n",
        "## You are working as a Data Scientist at Amazon\n",
        "- Amazon wants to develop an automated system to categorize a clothing product into its master category\n",
        "    - Link of website to show demo :(https://www.amazon.in/b/?_encoding=UTF8&node=7459780031&ref_=sv_top_ap_mega_1)\n",
        "- You are given the job to classify clothes into 10 different classes\n",
        "- Since it's a multi-class classification problem on a labelled dataset and you have just studied about ANN, you feel encouraged to apply it to this scenario.\n",
        "- You try to implement a ANN but somehow, it gives very poor results\n",
        "- So why didn't the ANN work..? and what else would ?\n",
        "\n",
        "Let's find out -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpqtGofIbcgw"
      },
      "source": [
        "# Part 1: (Hammering the Screw)  \n",
        "  \n",
        "  Scenario:  **Classify the fashion image into one of the 10 categories**  \n",
        "  Inventory: Vanilla NN (hammer) and CNN (new tool)\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1xL9UKISIdQPOSh3B_Z_ac3-8MzoW3jp7' width=1000 height=500>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhItXgSyb8gF"
      },
      "source": [
        "## 0. Setup.\n",
        "\n",
        "As always, we will start by downloading resources, installing packages and importing them to the environment!   \n",
        "- 0.1 Download Resources  \n",
        "- 0.2 Import Libraries  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNBy_m7jcXwj"
      },
      "source": [
        "### 0.1 Download Data\n",
        "\n",
        "We are going to use a collection of real-world clothing images dataset, opensourced [here](https://github.com/alexeygrigorev/clothing-dataset-small).  \n",
        "\n",
        "You can read more about the dataset in this [blog post](https://medium.com/data-science-insider/clothing-dataset-5b72cd7c3f1f)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6TYj_wcIlbL",
        "outputId": "6bb36f5e-ac85-47c8-8cd4-7e5134445c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: gdown: command not found\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1XdXz0TKo_KCDRHOMvzV-YtcTx7NPG-jC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NITwqXpBDLdD",
        "outputId": "083a65d1-af26-4d68-e89d-9dcdd38e681e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open /content/clothing-dataset-small.zip, /content/clothing-dataset-small.zip.zip or /content/clothing-dataset-small.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip /content/clothing-dataset-small.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXTYzYeBC6Wk"
      },
      "source": [
        "#### [Optional] Download trained checkpoints for this lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou8wDQiGBGAZ",
        "outputId": "b2c7ddea-93b4-4187-fb78-05404099993f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: gdown: command not found\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/drive/folders/1wgRokkvPUoGjsUZqJEhtpJn5e5lDTL4i --folder  # OPTIONAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haTCvFlxeFeM"
      },
      "source": [
        "### 0.2 Import Libraries\n",
        "\n",
        "a. viz libraries like matplotlib, seaborn etc.\n",
        "\n",
        "b. DL framework like tf, keras etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q1wcvODoeP-o"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmetrics\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
          ]
        }
      ],
      "source": [
        "# Import common libraries\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.metrics as metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Import tensorflow and its modules\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras # this allows <keras.> instead of <tf.keras.>\n",
        "from tensorflow.keras import layers # this allows <layers.> instead of <tf.keras.layers.>\n",
        "tf.keras.utils.set_random_seed(111) # set random seed\n",
        "\n",
        "# To supress any warnings during the flow\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivyc54lNe5MW"
      },
      "source": [
        "## 1. Data Visualization\n",
        "\n",
        "Q. **What to visualize?** Samples, Data Distribution etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrG3hEih-_rz"
      },
      "source": [
        "> We can see from the data directory that the data is divided into train/valid/test with each containing separate sub-folders for each class!  \n",
        "><img src='https://drive.google.com/uc?id=1sxY-Ej9Z3CXKaU6AyQwG6O4P7PRGWW2n' width=230>         \n",
        "\n",
        "#### **Now let's Plot Samples from each of these classes and Data Distribution across samples!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAGB3Gc4kYZH"
      },
      "outputs": [],
      "source": [
        "class_dirs = os.listdir(\"clothing-dataset-small/train\") # list all directories inside \"train\" folder\n",
        "image_dict = {} # dict to store image array(key) for every class(value)\n",
        "count_dict = {} # dict to store count of files(key) for every class(value)\n",
        "# iterate over all class_dirs\n",
        "for cls in class_dirs:\n",
        "    # get list of all paths inside the subdirectory\n",
        "    file_paths = glob.glob(f'clothing-dataset-small/train/{cls}/*')\n",
        "    # count number of files in each class and add it to count_dict\n",
        "    count_dict[cls] = len(file_paths)\n",
        "    # select random item from list of image paths\n",
        "    image_path = random.choice(file_paths)\n",
        "    # load image using keras utility function and save it in image_dict\n",
        "    image_dict[cls] = tf.keras.utils.load_img(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "id": "eKD4f585wslN",
        "outputId": "d6cbe75d-69eb-4253-8bf2-ca4d8e0d953e"
      },
      "outputs": [],
      "source": [
        "## Viz Random Sample from each class\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "# iterate over dictionary items (class label, image array)\n",
        "for i, (cls,img) in enumerate(image_dict.items()):\n",
        "    # create a subplot axis\n",
        "    ax = plt.subplot(3, 4, i + 1)\n",
        "    # plot each image\n",
        "    plt.imshow(img)\n",
        "    # set \"class name\" along with \"image size\" as title\n",
        "    plt.title(f'{cls}, {img.size}')\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV4HLrtSyIfO"
      },
      "source": [
        "- Notice that every Image has **different dimension!**\n",
        "- This is expected in a real-world data, but can you guess why this is **not ideal**? Hint: Batching requires all samples to be of the same size!\n",
        "- We will see later how we can handle this issue!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "2PycQ-NPw0S0",
        "outputId": "ce226c6a-fc44-4e1e-8613-585b56c37a2d"
      },
      "outputs": [],
      "source": [
        "## Let's now Plot the Data Distribution of Training Data across Classes\n",
        "df_count_train = pd.DataFrame({\n",
        "    \"class\": count_dict.keys(),     # keys of count_dict are class labels\n",
        "    \"count\": count_dict.values(),   # value of count_dict contain counts of each class\n",
        "})\n",
        "print(\"Count of training samples per class:\\n\", df_count_train)\n",
        "\n",
        "# draw a bar plot using pandas in-built plotting function\n",
        "df_count_train.plot.bar(x='class', y='count', title=\"Training Data Count per class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ROLUOAqyPcR"
      },
      "source": [
        "- Note that the `train/val/test` split already provided, we needn't create it explicitly!\n",
        "- Also, we can observe that the data is **imbalanced**. For this lecture, we will work with the given data-distribution, but later on we will see some common techniques to work with imbalanced-data! (Any thoughts?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2JQU8ou-vlz"
      },
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "### Q. **What type of Preprocessing is required here?**  \n",
        "  \n",
        "- 2.1 Shape Preprocessing  \n",
        "- 2.2 Value Preprocessing   \n",
        "- 2.3 Additional Shape Preprocessing for MLPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBoJ2jeslDMd"
      },
      "source": [
        "#### Before that, lets ***mount*** the data directory in Keras using [image_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) function\n",
        "\n",
        "Keras provides a utility function (`tf.keras.utils.image_dataset_from_directory`) to directly load data from directory in `tf.data.Dataset` format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muLrh0t4lCXi",
        "outputId": "70ed6522-f6a0-4e80-afb2-e38923030fd7"
      },
      "outputs": [],
      "source": [
        "print('\\nLoading Train Data...')\n",
        "train_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"clothing-dataset-small/train\", shuffle = True,\n",
        ")\n",
        "\n",
        "print('\\nLoading Validation Data...')\n",
        "val_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"clothing-dataset-small/validation\", shuffle = False,\n",
        ")\n",
        "\n",
        "print('\\nLoading Test Data...')\n",
        "test_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"clothing-dataset-small/test\", shuffle = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myAWBtDvCijS"
      },
      "source": [
        "### 2.1 Shape Preprocessing (Resizing)\n",
        "\n",
        "- The dataset may contain images of different sizes, but a batch in keras, requires all of them to be of the same size. (Why? , How?)\n",
        "- This can be achieved in a multiple of ways, but the most common technique is to resize all images to a fixed size (128,128)   \n",
        "\n",
        "*Note: We can also provide the resizing parameters while loading data itself.*\n",
        "\n",
        "### 2.2 Value Preprocessing (Rescaling)\n",
        "\n",
        "- We also convert the datatype of input tensor from `uint8` to `float32` and rescale the values to lie between 0 and 1 instead of 0 and 255. (Why?)\n",
        "- Without scaling, the high pixel range images will have large influence on loss values and thereby weight updation. (Eg: Black cat vs White cat)\n",
        "\n",
        "### 2.3 Shape Preprocessing for MLPs (Flatten)\n",
        "\n",
        "- Images are 2D* objects (with channel acting as an additional dimension)\n",
        "- But Neural Networks requires each sample to be 1D or flat!\n",
        "- Now Flattening of this matrix will lead to 3 elements of the vector to have value for that particular pixel as shown below.\n",
        "\n",
        "\n",
        "><img src='https://drive.google.com/uc?id=1LXJfVbDG23ro2t-7jZ1FC_kzXBT4Xfpw' height=600>\n",
        "\n",
        "- We will be using **Keras API** `Flatten layer` to convert the image matrix into vector\n",
        "- It will change `bs x 256 x 256 x 3` to `bs x 196608`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgkQxrRfGqCE"
      },
      "outputs": [],
      "source": [
        "height, width = 128, 128 # Optional, not required if we already have resized the data from `image_dataset_from_directory` function.\n",
        "\n",
        "# Data Processing Stage with resizing and rescaling operations\n",
        "data_preprocess_with_flatten = keras.Sequential(\n",
        "    name = \"data_preprocess_with_flatten\",\n",
        "    layers = [\n",
        "        layers.Resizing(height, width),\n",
        "        layers.Rescaling(1./255),\n",
        "        layers.Flatten(), # Note: We prefer NOT to add `layers.Flatten()` as part of preprocessing layers (rather as model definition itself)!\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Perform Data Processing on the train, val, test dataset\n",
        "train_ds = train_data.map(lambda x, y: (data_preprocess_with_flatten(x), y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGeTyNB9QVUX"
      },
      "source": [
        "Let's visualize a sample after preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTN6F8ZUQT4F",
        "outputId": "84a64e9d-c1f6-466b-fe34-e932eafc0935"
      },
      "outputs": [],
      "source": [
        "sample = next(iter(train_ds))[0]\n",
        "\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmDz1gC6PZom"
      },
      "source": [
        "- Its preferred to use flatten layer as part of **model definition**, instead of preprocessing stack, since it's a non-standard preprocessing step, which is not required while working with CNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grKUosJPPXBU"
      },
      "outputs": [],
      "source": [
        "# Data Processing Stage with resizing and rescaling operations\n",
        "data_preprocess = keras.Sequential(\n",
        "    name=\"data_preprocess\",\n",
        "    layers=[\n",
        "        layers.Resizing(height, width), # Shape Preprocessing\n",
        "        layers.Rescaling(1.0/255), # Value Preprocessing\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Perform Data Processing on the train, val, test dataset\n",
        "train_ds = train_data.map(lambda x, y: (data_preprocess(x), y))\n",
        "val_ds = val_data.map(lambda x, y: (data_preprocess(x), y))\n",
        "test_ds = test_data.map(lambda x, y: (data_preprocess(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3EusZbZkd3I"
      },
      "source": [
        "# **Quiz 1.** A ___ is a preprocessing layer which rescales input values to a new range.<br>\n",
        "a) Batch Normalization<br>\n",
        "b) Residual Layer<br>\n",
        "c) Rescaling<br>\n",
        "d) Flatten<br>\n",
        "<br>\n",
        "\n",
        "Ans : c)\n",
        "Before feeding into a neural network, we often rescale the values of image pixels to lie between 0 and 1.<br>\n",
        "This layer rescales every value of an input (often an image) by multiplying by scale and adding offset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYSHXG9AO7HF"
      },
      "source": [
        "## 3. Training Neural Network\n",
        "\n",
        "We will start by training a Vanilla NN and then check its performance on test dataset, after this we will switch to CNNs and see how much performance improvement we can get out of it!\n",
        "\n",
        "- 3.1 Model Architecture of Vanilla NN\n",
        "- 3.2 Loss function and Optimizer  \n",
        "- 3.3 Training and Evaluation with Vanilla NN   \n",
        "- 3.4 Replacing Vanilla NN with CNNs  \n",
        "- 3.5 Training and Evaluation with CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl4Y2NQvYxMR"
      },
      "source": [
        "### 3.1 Model Architecture of Vanilla NN\n",
        "\n",
        "- Recall the Neural Network structure covered in earilier lecture,\n",
        "  \n",
        "- How Neural Network is made of an Input layer, few Hidden layers and an Output layer with each neuron training on a distinct set of features based on the previous layerâ€™s output.\n",
        "  \n",
        "- These complex connection (weight matrix W) between the neurons helps NN to recognize patterns in the data\n",
        "><img src='https://drive.google.com/uc?id=19BIzu4JiXHNqBgTNXc8PMk_qaYR2nFK5' height=350>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePqy3ztrO6Sv"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "hidden_size_1 = 1024\n",
        "hidden_size_2 = 256\n",
        "\n",
        "model = keras.Sequential(\n",
        "    name=\"model_ann\",\n",
        "    layers=[\n",
        "        layers.Flatten(input_shape=(height, width, 3)), # alternatively, input_shape=next(iter(train_ds))[0].shape[1:]\n",
        "        layers.Dense(units=hidden_size_1, activation='relu'), # hidden layer 1\n",
        "        layers.Dense(units=hidden_size_2, activation='relu'), # hidden layer 2\n",
        "        layers.Dense(units=num_classes, activation='softmax'), # output layer\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbBJRBrbY8kY"
      },
      "source": [
        "- So now that we have created the model,  \n",
        "Lets Visualize **the structure of the model and the number of parameters,** using `.summary()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o253WfkcZAJF",
        "outputId": "979e9984-8418-4024-c247-81be9a7de2ed"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psvw7YDhZCLQ"
      },
      "source": [
        "- Lets also **plot the model structure** in the form os a graph using `tf.keras.utils.plot_model` command!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "vXS_XMFSZFTY",
        "outputId": "e2aef8a8-4860-4abc-9aa3-50244cdd6aa3"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, to_file=\"model_ann.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRCcHRa8Z_zK"
      },
      "source": [
        "### **Q. We have loaded data and defined model architecture, is there something else we need to do, before we start training?**  \n",
        "\n",
        "We need loss functions and optimizers!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RWABG3RZlBF"
      },
      "source": [
        "### 3.2 Loss Function and Optimizer\n",
        "\n",
        "a. Which loss function to use for multi-class classification?\n",
        "\n",
        "- **Cross-Entropy loss function might work**\n",
        "\n",
        "Lets take an example:\n",
        "- Supposedly we have a **4 class image classification task of whether the object in the image belong to Dog, Cat, Horse or Cheetah**\n",
        "\n",
        "- And we train a NN layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYKdYiFF5VSv"
      },
      "source": [
        "<center>  <img src='https://drive.google.com/uc?id=1CCx2lgwGW2mTIZvbmi85NN1UpjlxRgXX' height=250></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6CycTeJa9ho"
      },
      "source": [
        "- Notice how **Softmax converts logits (Ouput values) into probabilities.**\n",
        "- The purpose of the **Cross-Entropy is to take these output probabilities (P) and measure the distance** from the truth values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gm038x259Bl"
      },
      "source": [
        "<center>  <img src='https://drive.google.com/uc?id=1yAJ_jb7yCUlnsvQ_eHxc1nKPPcCAdxlj' height=250></center>\n",
        "<center>  <img src='https://drive.google.com/uc?\n",
        "id=1mCOeR0XcleTmPjuw6xM7-yEYxIvbuGH3' height=120></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSxGBqGwbMLB"
      },
      "source": [
        "### b. And which Optimizer will work the best ?\n",
        "- **Adam optimizer** might work\n",
        "\n",
        "- **uses both the momentum algorithm to  and Root Mean Square Propagation** (RMSProp) to quickly **reach the global minima**\n",
        "- it is computationally efficient\n",
        "- little memory required  \n",
        "   \n",
        "Lets now compile the model with these loss function and optimizer -    \n",
        "*Note: We use `sparse_categorical_crossentropy` instead of `categorical_crossentropy`, because the labels are not one-hot encoded!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn0Sqs9eZklM"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pawPnsHzbqm0"
      },
      "source": [
        "### 3.3 Training and Evaluation with Vanilla NN   \n",
        "\n",
        "a. Let's train the model for 10 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jHjaZvrcBEv",
        "outputId": "f3ff3e0a-5075-4c03-80cb-7cf92f1b879e"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "model_fit = model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHoMTFldcMUD"
      },
      "source": [
        "### b. Did the training go well ?\n",
        "- Lets plot the training and validation loss and accuracy curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "rajx6fF-c2lq",
        "outputId": "78f70412-3654-43c2-eced-4b329a630940"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(15,5))\n",
        "ax = axes.ravel()\n",
        "\n",
        "#accuracy graph\n",
        "ax[0].plot(range(0,model_fit.params['epochs']), [acc * 100 for acc in model_fit.history['accuracy']], label='Train', color='b')\n",
        "ax[0].plot(range(0,model_fit.params['epochs']), [acc * 100 for acc in model_fit.history['val_accuracy']], label='Val', color='r')\n",
        "ax[0].set_title('Accuracy vs. epoch', fontsize=15)\n",
        "ax[0].set_ylabel('Accuracy', fontsize=15)\n",
        "ax[0].set_xlabel('epoch', fontsize=15)\n",
        "ax[0].legend()\n",
        "\n",
        "#loss graph\n",
        "ax[1].plot(range(0,model_fit.params['epochs']), model_fit.history['loss'], label='Train', color='b')\n",
        "ax[1].plot(range(0,model_fit.params['epochs']), model_fit.history['val_loss'], label='Val', color='r')\n",
        "ax[1].set_title('Loss vs. epoch', fontsize=15)\n",
        "ax[1].set_ylabel('Loss', fontsize=15)\n",
        "ax[1].set_xlabel('epoch', fontsize=15)\n",
        "ax[1].legend()\n",
        "\n",
        "#display the graph\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6JnBH3bdVca"
      },
      "source": [
        "b. Let's evaluate it on Test Dataset\n",
        "\n",
        "We will calculate accuracy and generate confustion-matrix on the Test Dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEjRmxNBd7bQ",
        "outputId": "4002deb0-452f-4675-8844-df8246c697e9"
      },
      "outputs": [],
      "source": [
        "# load model from pretrained checkpoints (optional)\n",
        "model.load_weights(\"/content/Saved Models/L1_ann_model.ckpt\")\n",
        "\n",
        "# run model prediction and obtain probabilities\n",
        "y_pred = model.predict(test_ds)\n",
        "\n",
        "# get list of predicted classes by taking argmax of the probabilities(y_pred)\n",
        "predicted_categories = tf.argmax(y_pred, axis=1)\n",
        "\n",
        "# get list of class names\n",
        "class_names = test_data.class_names\n",
        "\n",
        "# create list of all \"y\"s labels, by iterating over test dataset\n",
        "true_categories = tf.concat([y for x, y in test_ds], axis=0)\n",
        "\n",
        "# calculate accuracy\n",
        "test_acc = metrics.accuracy_score(true_categories, predicted_categories) * 100\n",
        "print(f'\\nTest Accuracy: {test_acc:.2f}%\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "f_H4FdSXeB2n",
        "outputId": "437f9722-8559-4714-e678-4d806d1cd1cc"
      },
      "outputs": [],
      "source": [
        "def ConfusionMatrix(model, ds, label_list):\n",
        "# Note: This logic doesn't work with shuffled datasets\n",
        "    # run model prediction and obtain probabilities\n",
        "    y_pred = model.predict(ds)\n",
        "    # get list of predicted classes by taking argmax of the probabilities(y_pred)\n",
        "    predicted_categories = tf.argmax(y_pred, axis=1)\n",
        "    # create list of all \"y\"s labels, by iterating over test dataset\n",
        "    true_categories = tf.concat([y for x, y in ds], axis=0)\n",
        "    # generate confusion matrix and plot it\n",
        "    cm = metrics.confusion_matrix(true_categories,predicted_categories) # last batch\n",
        "    sns.heatmap(cm, annot=True, xticklabels=label_list, yticklabels=label_list, cmap=\"YlGnBu\", fmt='g')\n",
        "    plt.show()\n",
        "\n",
        "ConfusionMatrix(model, test_ds, test_data.class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkakKmGXeVBm"
      },
      "source": [
        "c. Let's also plot few samples from test set along with their predicted softmax probabilities.\n",
        "\n",
        "- We plot the image and the predicted array (as bar plot) side-by-side.\n",
        "- The predicted array is the class probabilities (output from softmax layer) corresponding to 10 classes.\n",
        "- If the tallest bar (argmax) in the bar-plot is the true label, we color it blue else it's colored red.\n",
        "- Height of the bar plot indicates what's the probability of it predicting that label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNPWQRkFeiZn"
      },
      "outputs": [],
      "source": [
        "# function to plot image given image, its true label and class probabilities (pred_array)\n",
        "def plot_image(pred_array, true_label, img):\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(pred_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(pred_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "# function to plot barplot of class probabilities (pred_array)\n",
        "def plot_value_array(pred_array, true_label):\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), pred_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(pred_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "96wzSiEmekXz",
        "outputId": "44eec017-c823-4cbb-cfc6-c9510fcb4378"
      },
      "outputs": [],
      "source": [
        "true_categories = tf.concat([y for x, y in test_ds], axis=0)\n",
        "images = tf.concat([x for x, y in test_ds], axis=0)\n",
        "y_pred = model.predict(test_ds)\n",
        "class_names = test_data.class_names\n",
        "\n",
        "# Randomly sample 15 test images and plot it with their predicted labels, and the true labels.\n",
        "indices = random.sample(range(len(images)), 15)\n",
        "# Color correct predictions in blue and incorrect predictions in red.\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i,index in enumerate(indices):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(y_pred[index], true_categories[index], images[index])\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(y_pred[index], true_categories[index])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCPhfqtTevqd"
      },
      "source": [
        "### 3.4 Replacing Vanilla NN with CNNs  \n",
        "\n",
        "- We saw hammering image data doesn't yield good results. Let's find the right set of tool that would do its job, aka CNNs.\n",
        "- Let's replace the first dense layer with a simple convolution block, keeping everything else same.  \n",
        "- A typical Convolution Block comprises of a `Conv2D` layer followed by a `MaxPooling2D` layer!    \n",
        "\n",
        "*(Don't worry if you don't understand what all of this means, it will be explained in detail in later sections)*\n",
        "\n",
        "a. Model Architecture  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0qK_UFZfRpK"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "hidden_size = 256\n",
        "\n",
        "model = keras.Sequential(\n",
        "    name=\"model_cnn\",\n",
        "    layers=[\n",
        "        layers.Conv2D(filters=16, kernel_size=3, strides=1, padding=\"same\", activation='relu', input_shape=(height, width, 3)),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(units=hidden_size, activation='relu'),\n",
        "        layers.Dense(units=num_classes, activation='softmax')\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "iK1qYZ3WfUsz",
        "outputId": "2da3065c-254e-4c7c-eb98-a1a6ae440328"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, to_file=\"model_cnn.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MgJ9MCnfWe9",
        "outputId": "1b66a029-2054-469e-a416-c168c19b3820"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d0dAJajfYyY"
      },
      "source": [
        "b. Compile the model with **cross-entropy** loss and **adam** optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MCeFdT9fhWG"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUxFWqScfncw"
      },
      "source": [
        "### 3.5 Training and Evaluation with CNNs\n",
        "\n",
        "a. Training for same 10 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXOH38S-f6tb",
        "outputId": "fdd9489f-f945-46aa-8494-21d73e32aa8e"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "model_fit = model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5ItzZi7f_n6"
      },
      "source": [
        "b. How well the training go?  \n",
        "\n",
        "Accuracy and Loss Plots on train/val datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "CYyrZVW2gIpa",
        "outputId": "1384fd03-3156-4602-8b26-f81d4fb8848d"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(15, 5))\n",
        "ax = axes.ravel()\n",
        "\n",
        "#accuracy graph\n",
        "ax[0].plot(range(0,model_fit.params['epochs']), [acc * 100 for acc in model_fit.history['accuracy']], label='Train', color='b')\n",
        "ax[0].plot(range(0,model_fit.params['epochs']), [acc * 100 for acc in model_fit.history['val_accuracy']], label='Val', color='r')\n",
        "ax[0].set_title('Accuracy vs. epoch', fontsize=15)\n",
        "ax[0].set_ylabel('Accuracy', fontsize=15)\n",
        "ax[0].set_xlabel('epoch', fontsize=15)\n",
        "ax[0].legend()\n",
        "\n",
        "#loss graph\n",
        "ax[1].plot(range(0,model_fit.params['epochs']), model_fit.history['loss'], label='Train', color='b')\n",
        "ax[1].plot(range(0,model_fit.params['epochs']), model_fit.history['val_loss'], label='Val', color='r')\n",
        "ax[1].set_title('Loss vs. epoch', fontsize=15)\n",
        "ax[1].set_ylabel('Loss', fontsize=15)\n",
        "ax[1].set_xlabel('epoch', fontsize=15)\n",
        "ax[1].legend()\n",
        "\n",
        "#display the graph\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2rwgcDQgMQL"
      },
      "source": [
        "c. Evaluation on Test Dataset  \n",
        "\n",
        "Print Classification Report, Confusion Matrix and plot a few samples (with their probabilities)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvZLbpDegZ5b",
        "outputId": "3fb26f2f-186c-4ea6-e322-9be6a558be20"
      },
      "outputs": [],
      "source": [
        "# load model from pretrained checkpoints (optional)\n",
        "model.load_weights(\"/content/Saved Models/L1_cnn_model.ckpt\")\n",
        "\n",
        "true_categories = tf.concat([y for x, y in test_ds], axis=0)\n",
        "images = tf.concat([x for x, y in test_ds], axis=0)\n",
        "y_pred = model.predict(test_ds)\n",
        "class_names = test_data.class_names\n",
        "predicted_categories = tf.argmax(y_pred, axis=1)\n",
        "\n",
        "test_acc = metrics.accuracy_score(true_categories, predicted_categories) * 100\n",
        "print(f'\\nTest Accuracy: {test_acc:.2f}%\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "3-_fTzi0gdY3",
        "outputId": "29367045-9cd5-4acd-bd88-3a68a6d59c71"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrix(model, test_ds, test_data.class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "OIjn2avvgfJC",
        "outputId": "601c1477-d7ac-48af-b6cf-9536ea5402ca"
      },
      "outputs": [],
      "source": [
        "true_categories = tf.concat([y for x, y in test_ds], axis=0)\n",
        "images = tf.concat([x for x, y in test_ds], axis=0)\n",
        "y_pred = model.predict(test_ds)\n",
        "class_names = test_data.class_names\n",
        "\n",
        "# Randomly sample 15 test images and plot it with their predicted labels, and the true labels.\n",
        "indices = random.sample(range(len(images)), 15)\n",
        "# Color correct predictions in blue and incorrect predictions in red.\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i,index in enumerate(indices):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(y_pred[index], true_categories[index], images[index])\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(y_pred[index], true_categories[index])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYfLz-8-gk74"
      },
      "source": [
        "**We see a BIG jump in accuracy numbers** both for validation and test sets. Therefore, where we have screw aka image data, using a specialist tool like screw-driver (CNNs) instead of a hammer (vanilla NN) proves to be more effective!\n",
        "\n",
        "- In the next section we will investigate why Vanilla NN performed poorly on image data, what special properties CNN possess and how CNN architecture is different than that of Vanilla NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6tJ8IPkE5TF"
      },
      "source": [
        "# PART 2: Understanding the Screwdriver (CNN)\n",
        "\n",
        "Now that we tried to hammer(ANN) a Screw (Image data) i.e. fit our data to a ANN, we observed :\n",
        "- ANN gave an test accuracy of 27% on the data\n",
        "- By adding a single Covolutional and MaxPool layer to the existing network,\n",
        "- we achieved 51% (nearly double) test accuracy\n",
        "\n",
        ">This raises 2 IMPORTANT questions:\n",
        "1.   #### **Why did the *Hammer* (MLP) FAIL in this *scenario* (Image Data)?**\n",
        "2.   #### **What SPECIAL Features helped Screwdriver (CNN) to succeed?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pddZzW-d6bLp"
      },
      "source": [
        "#### Let's address them one by one:\n",
        "\n",
        "## 1. **What are the problems with MLP for Image Data?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME8VLPVGTTeZ"
      },
      "source": [
        "### a. MLP will **react differently to an image and its shifted version**\n",
        "- Since **MLP flattens the image, it is not position invariant**\n",
        "- In PART 1, we fitted our image to ANN\n",
        "- Converted the image into a single feature vector,\n",
        "- hence **not considering** the neighbouring pixels, and\n",
        "- most importantly the Image channels (R-G-B)\n",
        "\n",
        "Lets take an example\n",
        "\n",
        "- Supposedly we have **two images of the same dog but at two different position**\n",
        "- One on the Upper left while one on the middle right\n",
        "\n",
        "><img src='https://drive.google.com/uc?id=1s0OD8Zdh0J7yFsvqvGeFEir5dzNCMjf_' height=200>                  <img src='https://drive.google.com/uc?id=1m_zIzbJGuXXTacENxTMeVb1dHvrpzb-E' height=200>\n",
        "\n",
        "- Now since MLP will **flatten the matrix, the neurons which might be more active for the first image will be dormant for the second one**\n",
        "- Making **MLP think these two images having completely different objects**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpayIKeyEjgO"
      },
      "source": [
        "\n",
        "### b. **MLP doesnot consider Spatial relations**\n",
        "\n",
        "- Spatial Information (like if a Person is standing at the right side of the Car or The red car is on the left side of the blue bike) gets lost when image is flattened\n",
        "\n",
        "- Flattening also loses the internal representation of the 2D image.\n",
        "\n",
        "### c. **Includes too many Parameters**\n",
        "- Since **MLP is a fully connected model, it requires a neuron for every input pixel of the image**\n",
        "\n",
        "- Now Lets take an example with an image of size (1280 x 720) .\n",
        "  - For an image with **dimension as such the vector for the input layer becomes (921600 x 1)**. if a **Dense layer of 128 is used  then the number of parameters  equals = 921600*128.**\n",
        "  - This makes **MLP infeasible  for large image** and it may **cause overfitting.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t58uo4VyErZq"
      },
      "source": [
        "\n",
        "#### Do we even require global connectivity ?\n",
        "- The global connectivity  caused due to densely connected neurons leads to  more reduntant parameters which makes the MLP overfit\n",
        "\n",
        "### With all the above discussion we need:\n",
        "- to make the system translation (position) invariant\n",
        "- to leverage the spatial correlation between the pixels\n",
        "- focus only on the local connectivity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y87hLI8mpcNt"
      },
      "source": [
        "## **Quiz 2.** Which of the following statements is False with regards to MLP for image data as input ?\n",
        "<br>\n",
        "a) MLPâ€™s donâ€™t consider spatial relations in an image<br>\n",
        "b) MLPâ€™s usually have lesser parameters when compared to CNNâ€™s because they are fully connected.<br>\n",
        "c) CNNâ€™s are preferred over MLPâ€™s for image data<br>\n",
        "d) None of the above<br>\n",
        "<br><br>\n",
        "\n",
        "**Ans** : b)<br>\n",
        "In MLPâ€™s,  Each node is connected to each node in the previous and the next layer, which causes it to have a very high number of parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b411bWfBgVN"
      },
      "source": [
        "\n",
        "## **2. What should be the SPECIAL Features of CNN?**\n",
        "\n",
        "From the above discussion and taking inspiration from our visual cortex system, there are 3 essential properties of image data:\n",
        "\n",
        "**1. LOCALITY** : Correlation between neighbouring pixels in a Image  \n",
        "**2. STATIONARITY** : Similar Patterns appearing multiple times in a Image  \n",
        "**3. COMPOSITIONALITY** : Extracting higher level features by pooling lower level features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2lDGK7AV-3W"
      },
      "source": [
        "### **Can we add these properties to Vanilla NN? If yes, How?** [Optional]\n",
        "\n",
        "- **Sparse Connections for Locality** - Since we already know that in the case of images, local features are highly correlated as opposed to far-away features, lets get rid of the far-away connections and make it sparse as shown below!   \n",
        "This restricts the Receptive Field (RF) of the neurons from 5RF to 3RF for the next hidden layer (3RF)!\n",
        "><img src='https://drive.google.com/uc?id=1z9RR-NHbJXnb2Vuxc_I0v-4XwX_vDnYB' height=320>\n",
        "<img src='https://drive.google.com/uc?id=10Frr3nGMeqvEVe3kiyoKq7pXwH2nLkql' height=380>\n",
        "    \n",
        "    *Note that the Receptive field of the hidden layers wrt to the initial layers increases as we go deeper!*\n",
        "\n",
        "- **Parameter Sharing for Stationarity** - Also, since there are repeating patterns in an image, it could be helpful to share some connections as shown below.\n",
        "><img src='https://drive.google.com/uc?id=1Fgp7vqGusF4vNScha7dvCUe2D-tRkoz0' height=250 width=700>  \n",
        "\n",
        "\n",
        "- Both these operations combined, greatly reduces the number of parameters in the architecture while preserving the \"inductive bias\" in the input signal. ***Note that, unlike in MLP; here, the number of parameters is now independent of number of neurons in the previous layer!***\n",
        "\n",
        "####In 2D, these two operations lead to a set of learnable parameters of size **k*k** (called Filters or Kernels) and the computations performed by these kernels are called **CONVOLUTION**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i2oCHFv537p"
      },
      "source": [
        "##Components in CNN:\n",
        "\n",
        "### Q. What is a **Convolution Operation**?\n",
        "In terms of deep learning, an (image) convolution is an element-wise multiplication of two matrices followed by a sum.\n",
        "1. Take two matrices (which both have the same dimensions).\n",
        "2. Multiply them, element-by-element (i.e., not the dot product, just a simple multiplication).\n",
        "3. Sum the elements together.\n",
        "\n",
        "Weâ€™ll learn more about convolutions, kernels, and how they are used inside CNNs in the remainder of this section.\n",
        "\n",
        "\n",
        "#### Now we will see how this kernel will work in the case of grayscale image or 2D matrix -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxS7QFRzG8UZ"
      },
      "source": [
        "### What is a 2D Kernel ?\n",
        "\n",
        "><img src='https://drive.google.com/uc?id=1mQ95EJrwQ_5LoGGiAU-9pT29V0hiM5fg' height=350> <img src='https://drive.google.com/uc?id=1h5uJOQyvN2GQheJF9ost1uq6SBj9Lyjz' height=350>\n",
        "\n",
        "\n",
        "\n",
        "- A kernel can be visualized as a small matrix that slides across, from left-to-right and top-to-bottom, of a larger image.\n",
        "- At each pixel in the input image, the neighborhood of the image\n",
        "is convolved with the kernel and the output stored.\n",
        "- Inshort, a kernel find relations within the neighbouring pixels in its area iteratively and once the finishes sliding over the image, it is able to correlate certain features in a image\n",
        "- For example :\n",
        "    - We have an image of a car on a road, the kernel when sliding over the image wil be able to identify the edges (shape) of a car, by correlating the pixels between the car and the road.\n",
        "### **Let's see how can we make Computer to detect vertical edges ?**\n",
        "\n",
        "><img src='https://drive.google.com/uc?id=1OoNlFfmCNyky1bXHDmc4KpWoJiPCqJDY' height=250>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### By using APPROPRIATE kernels/filters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2narADxBA2lM"
      },
      "source": [
        "Let's try to apply the Sobel vertical edge filter ourselves on the image above and see the results !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCHxH-RQ-ptR",
        "outputId": "3e6b7ec2-c712-42f9-a742-12fda429625d"
      },
      "outputs": [],
      "source": [
        "# Downloading the image\n",
        "!gdown 1TE-0GfSsWXaau_sMwO5_RgB84SSylOyk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "xmFmicNUBOVq",
        "outputId": "557f17e3-6b06-49db-fccb-89a1cef0630b"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import cv2\n",
        "from google.colab.patches import cv_imshow\n",
        "\n",
        "# Reading the image file using OpenCV\n",
        "img = cv2.imread('/content/cycle_people.png')\n",
        "\n",
        "# Displaying the input image\n",
        "cv_imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8w0__sZBz_e"
      },
      "source": [
        "Let's now use the cv2.Sobel() function to apply the Sobel vertical edge filter on the above image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "QgqCEGToBxl5",
        "outputId": "d2ba8cbf-88df-416b-9e0b-12f780fecee3"
      },
      "outputs": [],
      "source": [
        "# Apply the Sobel filter\n",
        "\n",
        "# The ddpeth parameter specifies the desired depth of the output image. In this case, it is set to 64-bit floating point values.\n",
        "# dx and dy parameters specify the order of derivative to be taken in the x and y directions respectively.\n",
        "# kzise parameter specifies the size of the Sobel filter\n",
        "output = cv2.Sobel(img, ddepth = cv2.CV_64F, dx = 1, dy = 0, ksize = 3)\n",
        "\n",
        "# Displaying the output image\n",
        "cv_imshow(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0moiVKbmIszh"
      },
      "source": [
        "Let's take an example of grayscale or 2d or  1-channel Input Image and a given 3x3 kernel -\n",
        "- Here **10 denotes a bright pixel while 0 denotes a dark one**\n",
        "- Let's now convolve the kernel on the Image to see what we get as output:\n",
        "\n",
        "><img src='https://drive.google.com/uc?id=1Nmco7hrrBxtSkpzCXi0Lji3uOjCerLnc' height=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouh70IMvIwrL"
      },
      "source": [
        "- See how the **top element of the kernel captures brighter pixels** and the **darkest pixels can be captured by the bottom elements**\n",
        "\n",
        "- Now we will convolve or traverse the kernel to each of the selected region **(highlighted in pink in image above)**\n",
        "\n",
        "- **multiply each element to the kernel/filter element** and then **sum them all**\n",
        "\n",
        "- the **value comes out to be 0** which forms the first element of the output matrix.   \n",
        "\n",
        "- Now the **kernel then shifts one column to the right** and again computes the operation. The amount of shifting is also called `stride`. In this case the stride is 1 both in horizontal and vertical direction. If the kernel shifted by two column towards right and only one towards botton, stride would have been (2,1).\n",
        "\n",
        "- Notice **after some computing**, the **output matrix was able to detect the horizontal line** which separated the bright and dark object.\n",
        "\n",
        "- Also this **shifting of kernel/filter help make the image translation invariant** as **no matter where the horizontal line is**, the **output matrix will have the same pixel values for the horizontal lines.**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QoSFQLzI3Ay"
      },
      "source": [
        "####Q. **What kind of filter do you think we can use for detecting vertical edges?**\n",
        "><img src='https://drive.google.com/uc?id=1lMLZjq7awxvRUbdedUoFxyIl3WmQLlMQ' height=220>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "478K1oTxfS9E"
      },
      "source": [
        "####Q. **How can we apply both horizontal and vertical edge detector on the same image matrix?**\n",
        "\n",
        "- By stacking Output from both the filters applied separately to the image!  \n",
        "- Notice that there is now a third Dimension to the Output Image, which is equal to the number of filters.\n",
        "\n",
        "> <img src='https://drive.google.com/uc?id=1udcOi7-MLlSVq0NEY7M5ASj3qNUVC3KI' height=300 width=1000>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKxTE55Qgfd6"
      },
      "source": [
        "####Q. **What happens when we have a RGB Image as Input?**\n",
        "\n",
        "- If the Input Image has more than 1 channel, each Kernel will have a depth associated with it (which is equal to the number of channels in Input Image) and the Convolution will be performed over the entire **VOLUME** of the input image as show here:\n",
        "> <img src='https://drive.google.com/uc?id=1QeT8Ot0QCSSE0zTwFz05K0nNF4DaP6a1' height=270>\n",
        "\n",
        "\n",
        "- In the case of 2 filters, we get an output dimension of 4x4x2.\n",
        "> <img src='https://drive.google.com/uc?id=1_SrrE9GLYrb1660H0MSDar8eSSmLF4vh' height=300>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0USYRhkPh66R"
      },
      "source": [
        "#### Q. **Are these two Kernels enough to know?**\n",
        "\n",
        "- Until now, we have seen just two kernels, Horizontal and Vertical; but there can be arbitrarily many such kernels which extracts useful features from image. Which all kernels to keep depends on the type of problem and datasets, we are dealing with.   \n",
        "- But in the case of Deep Learning, we don't handcraft feature extractors, rather we let the model learn them using **Gradient Descent**!  \n",
        "- Similarly here too, we will just specify the dimension and number of kernels we want, and let the model learn the values in each of those kernel.\n",
        "- These learnable kernels essentially form a **Conv Layer**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V2bfJX6nCSB"
      },
      "source": [
        "#### Q. **If these Kernels are learnable, how many number of parameters are there in a Convolution Layer?**\n",
        "\n",
        "- Number of parameters in a Conv Layer depends on the filter size, no. of filters, and the input image dimension.\n",
        "\n",
        "- Thus for every filter, we have `(m * n * d)+1` parameters, where m,n stands for width and height of the filter and d is the number of channels in previous layer. Additional 1 bias term is also added.\n",
        "- In total for `k` filters, we have `((m *n *d)+1)*k` parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W4Jna9PpoOc"
      },
      "source": [
        "#### Q. **Calculate the number of parameters in the Conv Layer of the CNN architecture (Part 1)?**\n",
        "\n",
        "- In the Conv Layer, total no. of params is just ((3 \\* 3 \\* 3) + 1) * 48 = 1334"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeYMG4PmxpTy"
      },
      "source": [
        "###Q. Did you notice that we have always used **Odd** Kernel Size of 3x3 instead of **Even** like 2x2 or 4x4? Why?\n",
        "\n",
        "- In Odd sized kernel, all the previous layer pixels would be Symmetric around the Output pixel\n",
        "- Without this Symmetric, we will account for distortion across all the layers which happen when we use Even sized kernel.\n",
        "\n",
        "Let's understand this with an example:\n",
        "- On the left, we have a 3Ã—3 matrix. The center of the matrix is located at\n",
        "x = 1, y = 1 where the top-left corner of the matrix is used as the origin and our coordinates are zero-indexed.\n",
        "- But on the right, we have a 2Ã—2 matrix. The center of this matrix would be located at x = 0.5, y = 0.5.\n",
        "- But as we know, there is no such thing as pixel location (0.5, 0.5) â€“ our pixel coordinates must be integers! This reasoning is exactly why we use odd kernel sizes i.e. to always ensure there is a valid (x, y)-coordinate at the center of the kernel.\n",
        "><img src='https://drive.google.com/uc?id=1kIUdSmmcBn4EZkiBt183KEKBXppAWG1p' height=300>\n",
        "  \n",
        "Thus, we use an odd kernel size to ensure there is a valid integer (x, y) coordinate at the center of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLytxuJ6fMSV"
      },
      "source": [
        "####One more thing, Did you notice that the output matrix has a shape of 4x4 instead of 6x6?\n",
        "\n",
        "###Q. **Why did the shape of the output matrix reduce ?**\n",
        "- The filter/kernel was shifting and computing over 3x3 region\n",
        "- So the area covered by the kernel/filter: `o = n-f+1`\n",
        "  - where o is the output dimension, n is the input dimension and f is the kernel dimension  \n",
        "- Hence the Output dimension of the matrix became: `6-3+1 = 4`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvqqiDausN2M"
      },
      "source": [
        "#### Q. **How can we solve the issues of shrinking output and less use of corner pixels?**\n",
        "\n",
        "- By **Padding or adding pixels around the border** of the image before applying convolution.\n",
        "\n",
        "#### Let's understand it in more detail:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zobjxhWsnXY"
      },
      "source": [
        "### Q. Why **Padding** is required in convolution?\n",
        "- By applying the convolution operation without padding, we not only get output which is smaller in shape but more importantly, we loose the information at the corner of the images.\n",
        "- The following diagram show that without padding the corner pixels are covered only 1 times, edge pixel are covered 2 times and all the remaining pixels are covered 3 times.\n",
        "><img src='https://drive.google.com/uc?id=1mQ95EJrwQ_5LoGGiAU-9pT29V0hiM5fg' height=350> <img src='https://drive.google.com/uc?id=1Fok3EPH7EKJ-e2lP5T45phfh_72aC8mK' height=350>\n",
        "\n",
        "By padding we ensure that all the pixels are covered equally as shown in the figure above. [Image]\n",
        "- The output shape, `o` is now given by;\n",
        "    `o = n+2p-f+1`, where p is the padding width applied at all sides.\n",
        "\n",
        "Alternative Explanation:\n",
        "- If we attempted to apply convolution at the pixel located at (0,0), then our 3Ã—3 kernel would **â€œhang offâ€** off the edge of the image.\n",
        "- Notice how there are no input image pixel values for\n",
        "the first row and first column of the kernel. Because of this, we always either start convolution at the first valid position\n",
        "<!-- <img src='https://drive.google.com/uc?id=10wB6bAxpVCod38m_AUL61idMTp5AuW6c' height=350> -->\n",
        "\n",
        "\n",
        "Thus padding is helpful because of following reasons:\n",
        "  1. It help us in preserving the information at the corner of the image. Without padding, the information at the corner of the image will be lost.\n",
        "  2. Without padding, the height $n_H$ and width $n_W$ decreases after the convolution operation. By using the padding, we can apply the convolution operation without decreasing the height and width of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64JuFdzQv1iI"
      },
      "source": [
        "####Q. **Is there another way to reduce the dimension of layer outputs and possibly control it?**\n",
        "\n",
        "Yes. **Through Stride Size.**\n",
        "\n",
        "- Strides define by how much the kernel will **slide** on the Input Image while Convolving.\n",
        "- Keeping a higher value of stride leads to quicker decrease in dimension of the Image. The image shown below shows stride of 2.\n",
        "\n",
        "> <img src='https://drive.google.com/uc?id=1mNgr-edMr3xu4DmFwJ5rZ410QW0rfxPm' height=300>\n",
        "\n",
        "- The new output shape, `o` is now equal to; `o = (n+2p-f)/s + 1`.  \n",
        "\n",
        "###Thus the formula for output shape is given by:\n",
        "> <img src='https://drive.google.com/uc?id=1uHepupWMJIHxTQ9PEIJ6TLu0Mc7VItvG' height=70>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bar6o1PfxatX"
      },
      "source": [
        "#### Q. **What is the effect of using a larger stride?**\n",
        "\n",
        "  - Lesser Memory needed for output\n",
        "  - The Output Spatial dimensions are smaller which when simplify the model\n",
        "  - It avoids Overfitting by not overlapping redundant pixels together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkeX0ypI61XX"
      },
      "source": [
        "### **Lets now revisit the code where we used Convolution layer and understand its arguments** -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc0Hk3ax7U7-",
        "outputId": "55f4ff4f-d5d2-428d-9e4f-03dd83ec0ee5"
      },
      "outputs": [],
      "source": [
        "layers.Conv2D(filters=48, kernel_size=3, strides=1, padding=\"same\", activation='relu', input_shape=(128, 128, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkJGuZp57V9N"
      },
      "source": [
        "Let's go over the arguments, one by one:\n",
        "1. `filters`: It denotes number of unique filters that this convolutional layers will learn. This also determines the number of output filters in the convolution.\n",
        "2. `kernel_size`: This specifies the size of the kernel/filter which will convolve over the previous layer. It can be either an integer value or a 2-tuple denoting the width and height of the 2D convolution window, like `(3,3)`, `(5,5)` etc.\n",
        "3. `strides`: Like kernel_size, we can specify the distance by which the kernel would slide over the input image in terms of integers or a tuple of 2 integers. Default is `(1,1)`.\n",
        "4. `padding`: As discussed, padding can be of two types, `same` or `valid`. Same keeps the output spatial dimensions same as the input, whereas valid means no padding, and allows the size to shrink as a result of convolution. Defaults to `valid`.\n",
        "5. `activation`: Added for convenience, this argument specifies what type of non-linearity should be applied *after convolution*! Defaults to `None`.\n",
        "6. `input_shape`: Need to be provided only if it is the first layer in the network! The input shape helps to determine the number of parameters in the layer and the output shape!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puT64UsBnihs"
      },
      "source": [
        "## **Quiz 3:** Which of the following are True about Padding in CNN?<br><bR>\n",
        "a) We should use valid padding if we know that information at edges is not that much useful.<br>\n",
        "b) We compromise to lose some edge information of the image in the case of â€˜sameâ€™ padding.<br>\n",
        "c) There is no reduction in dimension when we use â€˜sameâ€™ padding.<br>\n",
        "d) In â€™validâ€™ padding, we drop the part of the image where the filter does not fit.\n",
        "<br><br><br>Ans : a), c), d)\n",
        "<Br>\n",
        "**Solution :** <br>\n",
        "- We use valid padding if we know that information at edges is not that much useful.(The output size of the convolutional layer shrinks depending on the input size & kernel size.)\n",
        "- In zero padding, we pad zeros around the border of the image to save most of the information, whereas in valid padding, we lose out on the information which doesnâ€™t fit in filters.\n",
        "- There is no reduction in dimension when we use zero padding.(This is True)\n",
        "- In valid padding, we drop the part of the image where the filter does not fit. (This is True)\n",
        "<br><br>\n",
        "Hence, A, C and D are correct.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spjIQJ7reCXv"
      },
      "source": [
        "## **Pooling**\n",
        "   Apart from Conv2D, we also used another specific module called `MaxPooling2D` in our model, which had no learnable parameters.  \n",
        "\n",
        "#### Let's try to understand what is Pooling and what is the significance of a Pooling Module.  \n",
        "\n",
        "   The pooling layer helps in reducing the height and width of the image. Thus, it helps in reducing the number of parameters and also make invariant to the position in the input. In other words, it induces \"compositionality\" in our model architecture.\n",
        "\n",
        "   There are two kinds of pooling :   \n",
        "\n",
        "   1. Max pooling : The filter of size $(f,f)$ is slided over the input with a stride and we take the maximum in that region. It is used in between the Conv Layers.\n",
        "   \n",
        "   2. Average pooling : The filter of size $(f,f)$ is slided over the input with a sride and we take the average in that region. This type of pooling is generally used just before the Fully connected layers for feature extraction!\n",
        "\n",
        "\n",
        "><img src='https://drive.google.com/uc?id=1inJMPLbFxf5MyLRIOYHIjhIqEw_r31Ho' height=270> <img src='https://drive.google.com/uc?id=1TFIRlEtBxAjXqg-tFzj9hgXoWuEQb2Lc' height=270>\n",
        "\n",
        "\n",
        "  The above figure illustrates the max pooling and average pooling using a filter (2,2) dimension and having a stride of 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIg8k634zMLN"
      },
      "source": [
        "\n",
        "\n",
        "#### Q. **Does pooling layers add any learnable weights to the network?**\n",
        "- **No, since notice that no kernel/filter matrix is considered.**\n",
        "- Hence no parameters are needed while computing pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q4JAGBHUosn"
      },
      "source": [
        "#### Q. **What is the Output shape after pooling layer?**\n",
        "The output shape is given by: $o = floor[(\\frac{n-f}{s}) + 1]$, where n is the input size, f is the pooling filter size and s is the stride size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-RXnExM6s_A"
      },
      "source": [
        "### Let's now see How Pooling works in Keras?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQEstr5_6rGv",
        "outputId": "0bc500ed-2017-4428-9a76-ab893dfeb1d9"
      },
      "outputs": [],
      "source": [
        "layers.MaxPooling2D()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhZQeOU6W_v0",
        "outputId": "49e04ed8-9a58-43bf-9143-ea18e1754c58"
      },
      "outputs": [],
      "source": [
        "layers.AveragePooling2D(),"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFa4b69-fOw2"
      },
      "source": [
        "Both these Pooling layers have 2 important arguments:\n",
        "\n",
        "1. `pool_size`: This is the size of the window on which pooling will be applied. By default this size is 2. Can be an integer or a tuple of 2 integers. It can also be thought of as a non-learnable filter which either calculates max over the region or average out the values.\n",
        "2. `strides`: This is same as the one in `Conv2D` layer. Default is None, which means that the stride size is same as the pool size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PzsUoLypfR1"
      },
      "source": [
        "### Why we do flattening in CNN ?\n",
        "- This step is pretty simple, lets have a short discussion\n",
        "\n",
        "- After finishing the previous two steps, we're supposed to have a pooled feature map by now.\n",
        "- As the name of this step implies, we are literally going to flatten our pooled feature map into a column like in the image below.\n",
        "\n",
        "> <img src='https://drive.google.com/uc?id=1aIWkfcjMaEnj7hOYPSjlBoS4Pb0000Qf' height=260 width=620>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8VtT4eKSwSP"
      },
      "source": [
        "The reason we do this is that we're going to need to insert this data into an artificial neural network later on.\n",
        "\n",
        "><img src='https://drive.google.com/uc?id=17D_z-xzRSCJZK5OMsaVEJycMWnrTk2dV' height=270>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks70d_4oS9ar"
      },
      "source": [
        "As you see in the image above, we have multiple pooled feature maps from the previous step.\n",
        "\n",
        "What happens after the flattening step is that you end up with a long vector of input data that you then pass through the artificial neural network to have it processed further.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqdE8GBhlFxe"
      },
      "source": [
        "## **Quiz 4.** An input image has a matrix of size 12 X 12. A filter of size 3 X 3 with a Stride of 1 and no padding is applied on this image input. <br>Determine the size of the output matrix.\n",
        "\n",
        "a) 10 X 10<br>\n",
        "b) 12 x 12<br>\n",
        "c) 9 x 9<br>\n",
        "d) 10 x 12<br>\n",
        "<br>\n",
        "\n",
        "**Solution :** Use the formula : C = ((n-f+2p)/s)+1<br>\n",
        "Here, n = 12<br>\n",
        "F = 3<br>\n",
        "P = 0<br>\n",
        "S = 1<br>\n",
        "C = ((12-3+0)/1) + 1<br>\n",
        "\n",
        "Output size = 10 x 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1qdLbJ6TkfB"
      },
      "source": [
        "## Summary :\n",
        "\n",
        "To sum up, here is what we have after we're done with each of the steps in our first CNN that we have covered up until now:\n",
        "\n",
        "- Input image (starting point)\n",
        "- Convolutional layer (convolution operation)\n",
        "- Activation function\n",
        "- Pooling layer (pooling)\n",
        "- Input layer for the artificial neural network (flattening)\n",
        "- Dense layer\n",
        "- Loss function\n",
        "\n",
        "In the next class, we will discuss more about the CNN components  and how to deal with overfitting in CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jSmLzQfKsSO"
      },
      "source": [
        "# Dealing with Overfitting"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
